# -*- coding: utf-8 -*-
"""E2E MULTI AI AGENTS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OGYu9qK1_zQLoUlpBiv6CqQYTmoXf34g

## Why called multi AI agents?
Because one is interacting with the AstraDB, one is retrieving the results from there, one is communicating with the Wikipedia and retrieving the results and all are communicating with each other.
This is about end to end lang graph multi AI agents applications, where we have added conditional edges, we will see how each and every node functionality are created.
"""

!pip install langchain langgraph cassio

import cassio
## Connection of the Astra DB
ASTRA_DB_APPLICATION_TOKEN="AstraCS:EjBZzzZoBPBanZrUZWCtJBLv:06031de4037d9f442f71f0a9c6b7aeb10f25e572ce4a18c2065ce169f5338886"
ASTRA_DB_ID="f57ac56d-44e6-44c2-aa33-5b658ddeb3b2"
cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)

! pip install langchain_community

! pip install -U langchain_community tiktoken langchain-groq langchainhub chromadb langchain langgraph langchain_huggingface

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader ## Since i need to load the data from different web servers

urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

## load
docs = [WebBaseLoader(url).load() for url in urls]
doc_list = [item for sublist in docs for item in sublist]
print(doc_list)
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap = 0)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap=0)
docs_split = text_splitter.split_documents(doc_list)

docs_split

"""## NOW I HAVE TO CONVERT THIS INTO VECTORS USING SOME KIND OF EMBEDDINGS"""

from langchain_huggingface import HuggingFaceEmbeddings
embeddings =  HuggingFaceEmbeddings(model_name = "all-MiniLM-L6-v2")

from langchain.vectorstores.cassandra import Cassandra
astra_vector_store = Cassandra(embedding=embeddings,
                               table_name="qa_mini_demo",
                               session=None,
                               keyspace=None)

from langchain.indexes.vectorstore import VectorStoreIndexWrapper
astra_vector_store.add_documents(docs_split)
print("Inserted %i headlines." % len(docs_split))
astra_vector_index=VectorStoreIndexWrapper(vectorstore=astra_vector_store) ## This wrapper helps us to interact with the vector database

"""## Retriever is basically created on top of Astra Vector Store"""

retriever =  astra_vector_store.as_retriever()
retriever.invoke("What is agent")

"""## Langgraph Application(Router)"""

from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field ## We have to inherit thsi base model when we are using the route query

# Data model
class RouteQuery(BaseModel):
    """Route a user query to the most relevant datasource."""

    datasource: Literal["vectorstore", "wiki_search"] = Field(
        ...,
        description="Given a user question choose to route it to wikipedia or a vectorstore.",
    )

from langchain_groq import ChatGroq
from google.colab import userdata
import os
groq_api_key=userdata.get('groq_api_key')
print(groq_api_key)

llm = ChatGroq(groq_api_key=groq_api_key, model_name = "Llama-3.1-70b-Versatile")
llm

structured_llm_router=llm.with_structured_output(RouteQuery)

# Prompt
system = """You are an expert at routing a user question to a vectorstore or wikipedia.
The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.
Use the vectorstore for questions on these topics. Otherwise, use wiki-search."""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
question_router = route_prompt | structured_llm_router

print(question_router.invoke(
    {
        "question": "What is agent?"
    }
))

print(question_router.invoke(
    {
        "question": "Who is Shahrukh Khan?"
    }
))

! pip install langchain_community
! pip install wikipedia

from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.tools import WikipediaQueryRun
api_wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)
wiki = WikipediaQueryRun(api_wrapper = api_wrapper)

wiki.run("Tell me about Shahrukh Khan")

"""## AI AGENTS APPLICATION USING LANGGRAPH"""

## Graph

from typing import List

from typing_extensions import TypedDict


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
    """
    ## THESE 3 VARIABLES WILL BE MANAGING THE ENTIRE STATE OF AI AGENTS
    question: str
    generation: str
    documents: List[str]

from langchain.schema import Document


def retrieve(state):
    """
    Retrieve documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVE---")
    question = state["question"]

    # Retrieval
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question}

def wiki_search(state):
    """
    wiki search based on the re-phrased question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with appended web results
    """

    print("---wikipedia---")
    # print("---HELLO--")
    question = state["question"]
    print(question)

    # Wiki search
    docs = wiki.invoke({"query": question})
    #print(docs["summary"])
    wiki_results = docs
    wiki_results = Document(page_content=wiki_results)

    return {"documents": wiki_results, "question": question}

### Edges ###


def route_question(state):
    """
    Route question to wiki search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]
    source = question_router.invoke({"question": question})
    if source.datasource == "wiki_search":
        print("---ROUTE QUESTION TO Wiki SEARCH---")
        return "wiki_search"
    elif source.datasource == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

from langgraph.graph import END,StateGraph,START

workflow=StateGraph(GraphState)
# Define the nodes
workflow.add_node("wiki_search", wiki_search)  # web search
workflow.add_node("retrieve", retrieve)  # retrieve

# Build graph
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "vectorstore": "retrieve",
    },
)
workflow.add_edge( "retrieve", END)
workflow.add_edge( "wiki_search", END)
# Compile
app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

from pprint import pprint

# Run
inputs = {
    "question": "What is agent?"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'][0].dict()['metadata']['description'])

from pprint import pprint

# Run
inputs = {
    "question": "Avengers"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'])

